---
layout: post
title: "Optimization Schemes for Cache Directory Coherence Protocols in Multi-Core Architectures"
author: sam
categories: [ Theory ]
image: assets/images/cache/cover.avif
math: true
featured: false
hidden: true
---

In multi-core system architectures, cache coherence protocols are primarily classified into snooping protocols and directory protocols, with the latter generally being considered superior. In classic directory schemes, every data block has a corresponding directory entry, which results in significant storage overhead. Consequently, sparse directories can be employed to reduce costs by maintaining information only for data currently cached. The Cuckoo directory, which improves upon the sparse directory using hash tables, represents a robust solution. A potentially superior alternative is the Stash directory, which incorporates a flag bit in the directory to distinguish cache states, further minimizing performance loss. Both sparse directory schemes offer significant improvements over traditional directories. Based on theoretical analysis, their practical feasibility can be further verified through simulation.

## 1. Introduction

With the evolution of computer systems, multi-core and many-core architectures have become essential for achieving high performance and parallel computing. In multi-core architectures, the Cache serves as a critical component, playing a vital role in accelerating data access and enhancing system performance. However, read and write operations across private caches of multiple cores can lead to inconsistency issues. This has led to the development of numerous coherence protocols, among which the directory coherence protocol stands out for its scalability. Nevertheless, as the number of cores increases and shared data grows, traditional directory-based schemes face the challenge of excessive storage and access overheads. This article analyzes existing sparse directory schemes and their optimization effects on traditional directory-based protocols within the context of distributed storage in homogeneous multi-core architectures, and subsequently proposes further optimization methods.

This article first briefly introduces the basic principles of directory coherence protocols used in homogeneous multi-core architectures with distributed storage. It then elaborates on existing schemes that optimize directory structures into sparse directories and presents novel optimization results. Finally, it analyzes the advantages, disadvantages, and feasibility of each scheme.

## 2. Directory Coherence Protocols in Distributed Storage

The directory coherence protocol is an improvement over the snooping protocol, introducing a "directory" data structure that significantly enhances system scalability. The directory stores relevant information for each data block, making access operations more efficient and avoiding the performance limitations caused by the broadcast mechanism in snooping protocols. When using a classic directory coherence protocol in a distributed shared memory system, each data block has a dedicated entry in the directory. Based on this characteristic, we refer to this as a "Full Directory" for the sake of subsequent discussion.

Let us briefly analyze the overhead of a full directory. In this scheme, every node maintains complete directory information. Assuming each node's memory contains $M$ blocks and there are $N$ nodes in total, each full directory must record $M \times N$ directory entries. Consequently, the directory space overhead for the entire system reaches the order of $O(M \times N \times N)$. Due to the large number of processors, the storage overhead for the directory entries themselves is substantial. As the number of nodes $N$ increases, the space overhead grows quadratically, resulting in poor system scalability. Therefore, we consider reducing the number of directory entries in each node to mitigate the costs of a full directory. This leads to the introduction of the sparse directory as an improved solution.

## 3. Sparse Directory

### 3.1 Sparse Directory

The definition of a sparse directory was first proposed in 1992. It differs from a full directory in the way entries are allocated. A sparse directory allocates a directory entry only for cache blocks present in the Last Level Cache (LLC), recording the state information of the corresponding cache block. Since only a small fraction of blocks are loaded into various private caches at any given time, the vast majority of directory entries in a full directory are not accessed in the short term. Particularly as the core count increases and storage space expands, the proportion of these unused entries grows. Therefore, the storage overhead for these entries should be reduced. A sparse directory maintains consistency information only for data cached in private caches. Each directory entry includes a tag (indexing the cache block address), a state bit (recording the coherence state), and a sharing list (recording the distribution of data copies across processor cores) [1].

Sparse directories are characterized by low associativity and large directory capacity. The purpose of low associativity is to reduce power consumption during directory lookups, but this increases the probability of set conflicts and the frequency of directory entry replacements. Directory replacements, in turn, cause the corresponding cache blocks to become invalid, forcing all nodes sharing that cache block to perform replacements. This increases access latency and causes data thrashing, thereby degrading system performance. To minimize directory conflict frequency and avoid performance loss, an over-provisioned directory capacity is adopted, reducing intra-set access conflicts by increasing the number of directory sets.

### 3.2 Cuckoo Directory and Implementation

Ferdman et al. proposed using the Cuckoo directory to reduce the conflict replacements faced by sparse directories, aiming to solve the forced invalidation and cache thrashing issues caused by set conflicts in traditional schemes. This scheme retains the advantage of reduced storage overhead found in sparse directories while maintaining the stability of each node's cache to a great extent, making it a relatively ideal sparse directory solution.

The implementation of the Cuckoo directory is based on Cuckoo Hashing. The simplest sparse directory uses set associativity to reduce directory entries, which is essentially a form of hashing; therefore, improving sparse directories using hash tables is feasible. Cuckoo Hashing, also known as chained hashing, utilizes two or more parallel hash tables to reduce overhead when hash conflicts occur. Since the primary problem facing sparse directories after overhead reduction is conflict within set-associative directories, using chained hashing to mitigate conflicts is superior. In specific implementations, a 4-way Cuckoo directory structure can be adopted, using different hash functions for each way. When a new block becomes private storage and needs to be added to the directory, an insertion operation is triggered as follows:

1.  Calculate the results of the hash functions for each way.
2.  If a mapped position is empty, insert the entry there (if multiple ways have empty positions, one can be chosen arbitrarily).
3.  If none of the mapped positions are empty, select a way (starting from the first) to insert the new entry and evict the existing directory entry.
4.  Recalculate the hash function results for the evicted directory entry. If a mapped position is empty, execute step 2; otherwise, execute step 3.
5.  If the loop of steps 3 and 4 continues without resolving, stop re-mapping after a certain number of attempts and force the eviction of the entry. Since this effectively invalidates a block in the Cache, it has a certain negative impact on operational efficiency.

![4-way Cuckoo Hash Structure]({{site.baseurl}}/assets/images/cache/cuckoo-hash-structure.jpg){: .mx-auto .d-block}

When querying directory content, one simply maps the block index to be queried using the four-way hash and matches the index at the corresponding positions to determine a hit. Clearly, the time complexity for querying is $O(1)$. although the insertion time depends on the number of replacement cycles, amortized analysis shows that the amortized time for insertion is also $O(1)$ [2]. Thus, the Cuckoo directory possesses high time efficiency.

There are several reasons for adopting Cuckoo hashing here rather than a standard hash table. Firstly, while linear probing is the fastest method for resolving conflicts in general hash tables (proven to be 16%-23% faster than Cuckoo hashing [3]), the amortized cost of operations in Cuckoo hashing is $O(1)$, meaning it is not significantly slower in terms of rapid response. Secondly, linear probing tends to cause directory entries to cluster in certain areas, leading to hotspot issues. In contrast, the four-way structure of Cuckoo hashing distributes entries more uniformly across different ways, making it a data structure better suited for computer architecture design.

Therefore, the selection of hash functions largely determines the efficiency of the final implementation. The four hash functions must ensure excellent scattering properties so that mapping results are uniform. Simultaneously, due to the principle of locality, block addresses queried by a node within a period are relatively continuous. The hashing results should map continuous addresses to entries that are far apart within the same way; this is necessary to avoid hotspot issues.

It is also necessary to minimize the number of loops during replacement cycles; thus, the mapping results of the four-way hash should be distinct. Assuming an address space from 0 to N and N/10 directory entries per way, we analyzed and designed a function mapping method. Figure 2 illustrates the mapping result of this design for the 0-N/10 address space. Based on the principle of locality and the ratio of private cache to total storage, block numbers recorded in the directory at any given moment are generally concentrated within a relatively continuous area of size N/10. In this scenario, a mapping method with a slope of 1 uses different offsets to ensuring that each block has four scattered, distinct hash results. This utilizes scattered directory entries as much as possible at any moment, maximizing the reduction of hotspot issues.

![Hash Function Design]({{site.baseurl}}/assets/images/cache/hash-function-design.jpg){: .mx-auto .d-block}

### 3.3 Stash Directory

The Cuckoo directory successfully reduces conflicts and hotspots using multiple distinct hash functions. However, when insertion reaches a forced termination state, the final block is kicked out. Essentially, this fails to leverage the principle of locality and thus cannot completely avoid data thrashing. Consequently, Demetriades et al. proposed the Stash directory [4].

Consider a Cache block in an exclusive state; read and write operations on this block do not need to be restricted until another node requests it. The Stash directory exploits this characteristic by categorizing cached blocks into two distinct states: "Exclusive" and "Shared," adding a 1-bit flag in the directory to distinguish them. Typically, we use 0 for "Exclusive" and 1 for "Shared." From a hardware perspective, since any new directory entry added is initially in an exclusive state and the memory's initial state is generally 0, this bit can be recorded without extra operations, making the 0-1 correspondence logical.

Consider the case where a directory entry is replaced. If the flag indicates the block is in an "Exclusive" state, the block is not actually evicted from the Cache. This is because the node holding it exclusively will not cause consistency issues regardless of how it reads or writes, until another node requests that block. When another node calls for this block, since the record for it no longer exists in the directory, the system might erroneously conclude that the block is not in any node's private cache. This is known as a False Miss.

To avoid False Misses, a flag bit must be added to each Cache block in the shared LLC to indicate whether the corresponding Cache block has been cached by a lower-level Cache. This provides a new method to further determine the true state of each block.

![Shared LLC with False Miss Tag]({{site.baseurl}}/assets/images/cache/llc-false-miss-tag.jpg){: .mx-auto .d-block}

The Stash directory utilizes the shared LLC and coherence protocols to detect issues like False Misses. The shared LLC reserves a False Miss Tag bit for each cache block to indicate if the block might be "cached" at a lower level of the hierarchy. Whenever a directory entry is evicted without requiring the corresponding block in the Cache to be evicted (i.e., invalidated), this bit is set by a notification message sent from the directory. Therefore, following a miss in the directory, a hit in the LLC where the False Miss Tag is set will immediately identify this as an erroneous miss. Upon detecting a false miss, the LLC controller refuses to satisfy the miss and instead invokes the coherence protocol to issue a broadcast request to locate and use the latest copy of the block. Once the miss is resolved, the coherence protocol registers the block in the directory and resets the corresponding cache bit in the LLC. From this analysis, it is evident that the coherence protocol for the Stash directory undergoes significant changes compared to general protocols and requires specific adjustments during implementation.

As seen from the above, the Stash directory requires a shared LLC, which is common in modern Chip Multi-Processors (CMP). The LLC also requires extra bits to store the False Miss Tag. This additional storage is very small (<0.2% of cache size) and is independent of the number of cores; increasing the core count does not increase the extra storage for False Miss Tags. Conversely, the percentage of extra storage decreases as address length increases. It is also important to note that there must be an inclusion property between the blocks hidden by the directory and the corresponding LLC entries. Fully inclusive LLC designs are common in general multi-core system architectures, so this condition is easily met.

![Relationship between Shared LLC and Lower Level Cache Blocks]({{site.baseurl}}/assets/images/cache/llc-cache-relationship.jpg){: .mx-auto .d-block}

When an LLC block with the "cached" bit set is evicted, copies must be removed from the exclusive Cache hierarchy. Assuming an LLC entry with the cached bit set has no directory information, a background invalidation operation must be broadcast upon eviction. To avoid this, the cache system must implement clean eviction notifications (common in current commercial processors, such as AMD Opteron [5]). Therefore, when a block is evicted from the exclusive Cache hierarchy, a notification is sent to the directory, and the shared LLC is updated to better reflect the block's sharing state.

If the notification finds no corresponding entry in the directory, it implies the block being evicted is a hidden exclusive block currently tracked by the LLC. Thus, the eviction notification is forwarded to the LLC, which clears the cached state bit. Clearing eviction notifications may clear the cached state of LLC blocks before they are evicted, thereby reducing unnecessary broadcasts. Furthermore, allowing the LLC to prioritize replacing entries that have received eviction notifications can delay the eviction of cached LLC blocks. Further techniques to eliminate inclusion side effects can be found in previous studies [6] and are not the focus here.

As a conventional sparse directory, the Stash directory handles the eviction of entries tracking shared data blocks by forcing the invalidation of all cached copies. Although this may result in the simultaneous invalidation of blocks across multiple cores, it is preferable. This is because hiding shared blocks (especially migrating ones) significantly increases the frequency of false misses compared to private blocks.

Additionally, since the directory is sensitive to the temporal locality of actively shared blocks, victim shared entries (LRU) are likely tracking dead or temporarily exclusive shared blocks, thus benefiting from eviction. In the latter case, eviction gives the shared block a chance to reload as "Exclusive," and if it remains exclusive, it can re-enter the storage directory. The Stash directory leverages the temporal behavior of actively shared blocks while reducing the pollution of the directory set by truly exclusive items.

## 4. Feasibility Analysis

### 4.1 Theoretical Feasibility

Regarding the implementation of directory coherence protocols under the Full Directory, Traditional Set-Associative Sparse Directory, Cuckoo Directory, and Stash Directory structures described above, a comparison is made regarding space overhead, hotspot issues, and hardware implementation to establish theoretical feasibility.

<style>
.three-line-table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
.three-line-table thead {
  border-top: 2px solid #000;
  border-bottom: 1.5px solid #000;
}
.three-line-table tbody {
  border-bottom: 2px solid #000;
}
.three-line-table th,
.three-line-table td {
  padding: 8px 12px;
  text-align: center;
  border: none;
}
.three-line-table th {
  font-weight: bold;
}
</style>

<table class="three-line-table">
  <thead>
    <tr>
      <th>Directory Type</th>
      <th>Space Overhead</th>
      <th>Hotspot Issue</th>
      <th>Hardware Complexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Full Directory</td>
      <td>$O(M \times N^2)$</td>
      <td>None</td>
      <td>Simple</td>
    </tr>
    <tr>
      <td>Traditional Sparse Directory</td>
      <td>$O(M)$</td>
      <td>Severe</td>
      <td>Medium</td>
    </tr>
    <tr>
      <td>Cuckoo Directory</td>
      <td>$O(M)$</td>
      <td>Moderate</td>
      <td>Relatively Complex</td>
    </tr>
    <tr>
      <td>Stash Directory</td>
      <td>$O(M)$</td>
      <td>Minimal</td>
      <td>Complex</td>
    </tr>
  </tbody>
</table>

### 4.2 Experimental Feasibility

The time cost of using hardware circuit connections for simulation experiments is immense; therefore, simple software simulations should be conducted first. gem5 is a computer system architecture simulation platform running on Linux. We configured the gem5 experimental environment on Ubuntu and conducted simple experiments according to the official documentation [7]. gem5 provides two different Cache implementation methods, one of which, the Ruby simulator, allows us to custom configure directory structures, directory implementations, message passing, and coherence protocol details. We simulated a dual-CPU system with distributed storage architecture on gem5, where each CPU includes an L1 Cache, using the MSI directory coherence protocol for simple simulation. The platform can output a series of simulation results, such as total clock cycles based on the execution content. By running test programs with different characteristics and recording data such as total clock cycles and miss rates, we can compare mean values and standard deviations. Based on this more realistic data, we can perform a more complete feasibility verification of the aforementioned schemes.

![Simulation results including total clock cycles]({{site.baseurl}}/assets/images/cache/simulation-results.png){: .mx-auto .d-block}

## 5. Summary

Addressing the defect of large storage space overhead in traditional directories, we introduced the sparse directory for optimization. Among the various specific types of sparse directories, we first noted the Cuckoo directory, which utilizes hash tables; our research focused primarily on the four-way Cuckoo directory structure. Analysis indicates that under this structure, the time complexity of various operations is low, resulting in high time efficiency. Additionally, the efficiency of the Cuckoo directory depends on the selection of specific hash functions. Considering the principle of program locality and the issue of loop replacement, mapping results in the most scattered scenarios can effectively avoid hotspot issues, thereby achieving optimal performance. Subsequently, we researched the Stash directory, which holds even greater potential. The Stash directory classifies Cache block states into exclusive and shared categories, requiring an additional 1-bit flag in the directory for implementation. When directory replacement occurs, the flag bit determines if an operation is necessary, further improving performance. Finally, future research can further verify the practical effects of these schemes through simulation.

From this research, it can be observed that optimizations for computer system structures often adopt methods such as parallelism, scattering, and tagging. These methods either directly or indirectly establish multi-level relationships to achieve buffering, or expand the same level to obtain better parallel operation efficiency. Scattering and similar methods essentially aim to improve the efficiency of parallel processing. These three points can be utilized not only in the optimization of directory coherence protocols but also referenced in the optimization of other processes, achieving broad application.

## References

[1] Srivatsa Akshay; Fasfous Nael; Anh Vu Doan Nguyen, et al. Exploring a Hybrid Voting-based Eviction Policy for Caches and Sparse Directories on Manycore Architectures[J]. 2021,87(Nov.):104384.1-104384.17

[2] https://web.stanford.edu/class/archive/cs/cs166/cs166.1146/lectures/13/Small13.pdf

[3] Friedhelm Meyer auf der Heide. Algorithms â€” ESA 2001[M]. Springer, Berlin, Heidelberg, 2001

[4] Wu Jianguo, Chen Haiyan, Liu Sheng, Deng Rangyu, Chen Junjie. A Survey of Performance Improvement Methods for Multi-core Cache Sparse Directories[J]. Computer Engineering & Science, 2019, 41(03):385-392.

[5] P. Conway, N. Kalyanasundharam, G. Donley, K. Lepak, and B. Hughes. Cache hierarchy and memory subsystem of the amd opteron processor. IEEE Micro, 2010.

[6] A. Jaleel, E. Borch, M. Bhandaru, S. C. Steely Jr., and J. Emer. Achieving non-inclusive cache performance with inclusive caches: Temporal locality aware (tla) cache management policies. MICRO, 2010.

[7] https://www.gem5.org/documentation/learning_gem5/introduction/